# -*- coding: utf-8 -*-
"""Report 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uLyEHI9QPiIJgVjrd73SmcrmHbJzKCZn
"""

# Backpropagation from scratch

import numpy as np

# Variable class
class Variable:
    def __init__(self, data):
        self.data = data
        self.grad = None
        self.creator = None

    def set_creator(self, func):
        self.creator = func

    def backward(self):
        if self.grad is None:
            self.grad = np.array(1.0)

        funcs = [self.creator]
        while funcs:
            f = funcs.pop()
            x = f.input
            y = f.output
            x.grad = f.backward(y.grad)

            if x.creator is not None:
                funcs.append(x.creator)


# Function base class
class Function:
    def __call__(self, input):
        x = input.data
        y = self.forward(x)
        output = Variable(y)
        output.set_creator(self)
        self.input = input
        self.output = output
        return output

    def forward(self, x):
        raise NotImplementedError()

    def backward(self, gy):
        raise NotImplementedError()


# Specific Function classes: Square and Exp

class Square(Function):
    def forward(self, x):
        return x ** 2

    def backward(self, gy):
        x = self.input.data
        return 2 * x * gy

class Exp(Function):
    def forward(self, x):
        return np.exp(x)

    def backward(self, gy):
        x = self.input.data
        return np.exp(x) * gy

# Numerical differentiation function

def numerical_diff(f, x, eps=1e-4):
    x0 = Variable(x.data - eps)
    x1 = Variable(x.data + eps)
    y0 = f(x0)
    y1 = f(x1)
    return (y1.data - y0.data) / (2 * eps)


# Building and testing the computation graph

if __name__ == "__main__":
    # Create instances of functions
    A = Square()
    B = Exp()
    C = Square()

    # Create input variable
    x = Variable(np.array(0.5))

    # Forward pass
    a = A(x)
    b = B(a)
    y = C(b)

    # Backward pass
    y.backward()

    # Output result
    print("x.data:", x.data)
    print("x.grad:", x.grad)

# The model computes: y = (exp(x^2))^2
# Then performs backpropagation through the computational graph.