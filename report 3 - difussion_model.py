# -*- coding: utf-8 -*-
"""Difussion_Model_Exercise

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AcVi9qgFyKnVFrn1eSYZYJ8z0na1i_s0
"""

# Commented out IPython magic to ensure Python compatibility.
# Install package
# %pip install -q diffusers
!pip install torchvision

import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from diffusers import DDPMScheduler, UNet2DModel
from matplotlib import pyplot as plt

# Setup device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f'Using device: {device}')

# Load dataset (MNIST)
dataset = torchvision.datasets.MNIST(root="mnist/", train=True, download=True,
                                     transform=torchvision.transforms.ToTensor())

train_dataloader = DataLoader(dataset, batch_size=64, shuffle=True)  # Increased Batch Size

# Corruption function  Gaussian noise
def corrupt(x, amount):
    """Corrupt the input `x` by mixing it with Gaussian noise according to `amount`."""
    amount = amount.view(-1, 1, 1, 1)  # Broadcast amount
    noise = torch.randn_like(x)  #  Gaussian noise
    return x * (1 - amount) + noise * amount, noise

# Visualizing Original Data and After Corruption
x, y = next(iter(train_dataloader))
amount = torch.linspace(0, 1, x.shape[0], device=x.device).view(-1, 1, 1, 1)
noised_x, noise = corrupt(x, amount)

fig, axs = plt.subplots(2, 1, figsize=(12, 5))
axs[0].set_title('Input data')
axs[0].imshow(torchvision.utils.make_grid(x).permute(1, 2, 0).cpu().numpy(), cmap='Greys')

axs[1].set_title('Corrupted data (-- amount increases -->)')
axs[1].imshow(torchvision.utils.make_grid(noised_x).permute(1, 2, 0).cpu().numpy(), cmap='Greys')

plt.show()

# Using UNet2DModel from diffusers
model = UNet2DModel(
    sample_size=28,
    in_channels=1,
    out_channels=1,
    layers_per_block=2,
    block_out_channels=(32, 64, 128),
    down_block_types=("DownBlock2D", "AttnDownBlock2D", "AttnDownBlock2D"),
    up_block_types=("AttnUpBlock2D", "AttnUpBlock2D", "UpBlock2D")
).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
loss_fn = nn.MSELoss()
scheduler = DDPMScheduler(num_train_timesteps=1000)  # Adding scheduler

# Training loop
num_epochs = 5  # Adding more epoch
losses = []

for epoch in range(num_epochs):
    for x, _ in train_dataloader:
        x = x.to(device)
        noise_amount = torch.rand(x.shape[0]).to(device)
        noisy_x, noise = corrupt(x, noise_amount)  # Adding Corruption noise

        timestep = torch.randint(0, 1000, (x.shape[0],), device=device)  # Using timesteps
        pred = model(noisy_x, timestep).sample

        loss = loss_fn(pred, noise)  # Compare with original noise
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        losses.append(loss.item())

    avg_loss = sum(losses[-len(train_dataloader):]) / len(train_dataloader)
    print(f"Epoch {epoch+1} - Avg loss: {avg_loss:.5f}")

plt.plot(losses)
plt.title("Training Loss Over Time")
plt.xlabel("Iterations")
plt.ylabel("Loss")
plt.show()

# Sampling from the pure noise
x = torch.randn(64, 1, 28, 28).to(device)
for i in range(40):  # 40 sampling steps
    timestep = torch.tensor([999 - i], device=device).repeat(x.shape[0])
    pred = model(x, timestep).sample
    x = scheduler.step(pred, timestep, x).prev_sample

fig, ax = plt.subplots(1, 1, figsize=(12, 12))
ax.imshow(torchvision.utils.make_grid(x.detach().cpu(), nrow=8).permute(1, 2, 0).numpy(), cmap='Greys')
ax.set_title('Generated Samples')
plt.show()